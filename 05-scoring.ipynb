{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(gt_path, predictions_path, submission_id, extension=\".png\"):\n",
    "    results = pd.DataFrame(columns=[\"Image\", \"Threshold\", \"F1\", \"Jaccard\", \"TP\", \"FP\", \"FN\", \"Official_Score\", \"Precision\", \"Recall\"])\n",
    "    false_negatives = pd.DataFrame(columns=[\"False_Negative\", \"Area\"])\n",
    "    splits_merges = pd.DataFrame(columns=[\"Image_Name\", \"Merges\", \"Splits\"])\n",
    "\n",
    "    for image_name in y_true_identifiers:\n",
    "        # Load ground truth data\n",
    "        img_filename = os.path.join(gt_path, image_name + \".png\")\n",
    "        ground_truth = skimage.io.imread(img_filename)\n",
    "        \n",
    "        if len(ground_truth.shape) == 3:\n",
    "            ground_truth = ground_truth[:,:,0]\n",
    "\n",
    "        # Transform to label matrix\n",
    "        ground_truth = skimage.morphology.label(ground_truth)\n",
    "\n",
    "        # Load predictions\n",
    "        pred_filename = os.path.join(predictions_path, str(submission_id) + \"-\" + image_name + extension)\n",
    "        if os.path.isfile(pred_filename):\n",
    "            prediction = skimage.io.imread(pred_filename)\n",
    "        else:\n",
    "            prediction = np.zeros_like(ground_truth)\n",
    "\n",
    "        # Relabel objects\n",
    "        ground_truth = skimage.segmentation.relabel_sequential(ground_truth)[0] \n",
    "        prediction = skimage.segmentation.relabel_sequential(prediction)[0] \n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        results = evaluation.compute_af1_results(\n",
    "            ground_truth, \n",
    "            prediction, \n",
    "            results, \n",
    "            image_name\n",
    "        )\n",
    "\n",
    "        false_negatives = evaluation.get_false_negatives(\n",
    "            ground_truth, \n",
    "            prediction, \n",
    "            false_negatives, \n",
    "            image_name\n",
    "        )\n",
    "\n",
    "        splits_merges = evaluation.get_splits_and_merges(\n",
    "            ground_truth, \n",
    "            prediction, \n",
    "            splits_merges, \n",
    "            image_name\n",
    "        )\n",
    "        \n",
    "    return results, false_negatives, splits_merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(\"/storage/data/DSB2018/metadata/submissions.csv\")\n",
    "\n",
    "submissions = submissions[submissions[\"score\"] > 0]\n",
    "\n",
    "submissions[\"max_score\"] = submissions.groupby([\"team\"])[\"score\"].transform(max)\n",
    "\n",
    "submissions[\"best_submission\"] = submissions[\"max_score\"] == submissions[\"score\"]\n",
    "\n",
    "submissions = submissions[submissions[\"best_submission\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_pathnames = glob.glob(\"/storage/data/DSB2018/phase-2-gt/*.png\")\n",
    "\n",
    "y_true_identifiers = [os.path.splitext(os.path.basename(pathname))[0] for pathname in y_true_pathnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute scores for all submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/706 [00:00<?, ?it/s]/home/jccaicedo/dsb2018-analysis/evaluation.py:105: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  results = pd.concat([results, pd.DataFrame(data=data.T, columns=[\"Area\", \"False_Negative\"])])\n",
      "/home/jccaicedo/dsb2018-analysis/evaluation.py:68: RuntimeWarning: Mean of empty slice.\n",
      "  jaccard = np.max(IOU, axis=0).mean()\n",
      "/home/jccaicedo/.local/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "100%|██████████| 706/706 [3:48:25<00:00, 19.41s/it]  \n"
     ]
    }
   ],
   "source": [
    "accuracy = pd.DataFrame(columns=[\"Image\", \"Threshold\", \"F1\", \"Jaccard\", \"TP\", \"FP\", \"FN\", \"Official_Score\", \"SubmissionID\"])\n",
    "false_negatives = pd.DataFrame(columns=[\"False_Negative\", \"Area\", \"SubmissionID\"])\n",
    "splits_merges = pd.DataFrame(columns=[\"Image_Name\", \"Merges\", \"Splits\", \"SubmissionID\"])\n",
    "\n",
    "gt_path = \"/storage/data/DSB2018/phase-2-gt/\"\n",
    "predictions_path = \"/storage/data/DSB2018/phase-2-predictions/\"\n",
    "\n",
    "for idx in tqdm( submissions[\"team\"].unique() ):\n",
    "    record = submissions[submissions[\"team\"] == idx]\n",
    "    \n",
    "    # Take the first row. Repeated records have the same score\n",
    "    submission_id = record.iloc[0].id\n",
    "    accu, falsen, spl_mer = compute_scores(gt_path, predictions_path, submission_id)\n",
    "    accu[\"SubmissionID\"] = submission_id\n",
    "    falsen[\"SubmissionID\"] = submission_id\n",
    "    spl_mer[\"SubmissionID\"] = submission_id\n",
    "    \n",
    "    # Append results\n",
    "    accuracy = pd.concat([accuracy, accu])\n",
    "    false_negatives = pd.concat([false_negatives, falsen])\n",
    "    splits_merges = pd.concat([splits_merges, spl_mer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.to_csv(\"/storage/data/DSB2018/results/accuracy.csv\", index=False)\n",
    "false_negatives.to_csv(\"/storage/data/DSB2018/results/false_negatives.csv\", index=False)\n",
    "splits_merges.to_csv(\"/storage/data/DSB2018/results/splits_merges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellProfiler baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jccaicedo/dsb2018-analysis/evaluation.py:105: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  results = pd.concat([results, pd.DataFrame(data=data.T, columns=[\"Area\", \"False_Negative\"])])\n"
     ]
    }
   ],
   "source": [
    "gt_path = \"/storage/data/DSB2018/phase-2-gt/\"\n",
    "predictions_path = \"/storage/data/DSB2018/CP_segmentations/all/\"\n",
    "\n",
    "submission_id = \"cellprofiler\"\n",
    "accu, falsen, spl_mer = compute_scores(gt_path, predictions_path, submission_id, extension=\".tiff\")\n",
    "accu[\"SubmissionID\"] = submission_id\n",
    "falsen[\"SubmissionID\"] = submission_id\n",
    "spl_mer[\"SubmissionID\"] = submission_id\n",
    "\n",
    "accu.to_csv(\"/storage/data/DSB2018/results/cp_accuracy.csv\", index=False)\n",
    "falsen.to_csv(\"/storage/data/DSB2018/results/cp_false_negatives.csv\", index=False)\n",
    "spl_mer.to_csv(\"/storage/data/DSB2018/results/cp_splits_merges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
